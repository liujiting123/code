{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-17T06:22:56.254255Z",
     "start_time": "2025-04-17T06:22:44.419077Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch\n",
    "\n",
    "batch_size = 32\n",
    "d_model = 256\n",
    "d_ff = 512\n",
    "N_decoder_layers = 8\n",
    "N_encoder_layers = 4\n",
    "vocab_size = 23\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T06:22:56.315631Z",
     "start_time": "2025-04-17T06:22:56.274263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "src_mask: 用于 encoder 和 decoder 的 cross-attention\n",
    "tgt_mask: 用于 decoder 的 self-attention\n",
    "\"\"\""
   ],
   "id": "cbe032db54496d05",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsrc_mask: 用于 encoder 和 decoder 的 cross-attention\\ntgt_mask: 用于 decoder 的 self-attention\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T06:22:57.046398Z",
     "start_time": "2025-04-17T06:22:57.035282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_padding_mask(queries, keys):\n",
    "    \"\"\"\n",
    "    queries: (B,heads ,L_q, D)\n",
    "    keys:    (B,heads ,L_k, D)\n",
    "    return: padding_mask (B, heads, L_q, L_k)\n",
    "    \"\"\"\n",
    "    # 假设padding部分全为0,这里不为0的部分都是True\n",
    "    query_mask = torch.sum(queries, dim=-1) != 0  # shape: (B,heads, L_q) bool\n",
    "    key_mask = torch.sum(keys, dim=-1) != 0      # shape: (B,heads, L_k) bool\n",
    "\n",
    "    # 扩展维度以构造 (B, heads, L_q, L_k)\n",
    "    query_mask = query_mask.unsqueeze(3)         # (B,heads, L_q, 1)\n",
    "    key_mask = key_mask.unsqueeze(2)             # (B, heads,1, L_k)\n",
    "\n",
    "    # 只有 query 和 key 都是有效的，才是 True，True代表着有效\n",
    "    padding_mask = query_mask & key_mask         # (B, L_q, L_k)\n",
    "    padding_mask = ~padding_mask                 # 取反，mask位置为 True\n",
    "\n",
    "    return padding_mask           # (B, 1, L_q, L_k)"
   ],
   "id": "d880757efdb061ce",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### FFN层",
   "id": "966789c4e6729486"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T06:22:57.107682Z",
     "start_time": "2025-04-17T06:22:57.099448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.Linear(in_features, hidden_features)\n",
    "        self.dense2 = nn.Linear(hidden_features, out_features)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.dense1(x))\n",
    "        x = self.dense2(x)\n",
    "        return x  #-->batch_size,seq_length,d_model"
   ],
   "id": "364a8189dcf8bf67",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 多头注意力",
   "id": "c9cef159abcbd304"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T06:22:57.242564Z",
     "start_time": "2025-04-17T06:22:57.214564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#@save\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        assert (d_model % n_heads == 0)\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.linear_q = nn.Linear(d_model, d_model)\n",
    "        self.linear_k = nn.Linear(d_model, d_model)\n",
    "        self.linear_v = nn.Linear(d_model, d_model)\n",
    "        self.linear_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def _masked_attention(self, q, k, v, mask=None):\n",
    "        mask = get_padding_mask(q, k)\n",
    "        attention = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.d_k)  # (B, H, L_q, L_k)\n",
    "\n",
    "        # 构造因果掩码（causal_mask）\n",
    "        causal_mask = torch.triu(torch.ones(q.size(-2), k.size(-2), device=q.device), diagonal=1)\n",
    "        causal_mask = causal_mask.bool().unsqueeze(0).unsqueeze(1)  # (1, 1, L_q, L_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.bool()\n",
    "            # 广播：padding_mask (B, 1, L_q, L_k) | causal_mask (1, 1, L_q, L_k) -> (B, 1, L_q, L_k)\n",
    "            mask = mask | causal_mask\n",
    "        else:\n",
    "            mask = causal_mask  # 直接使用因果mask，广播给attention用\n",
    "\n",
    "        # 应用mask：mask为True的位置被设置为-1e9\n",
    "        attention = torch.where(mask, torch.tensor(-1e9, device=q.device), attention)\n",
    "\n",
    "        weights = torch.softmax(attention, dim=-1)\n",
    "        return torch.matmul(weights, v)  # (B, H, L_q, D)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        b,s_q,d = q.shape\n",
    "        _,s_k,_ = k.shape\n",
    "        assert (d == self.d_model)\n",
    "        queries = self.linear_q(q).reshape(b,s_q,-1,self.d_k).transpose(1, 2)\n",
    "        keys = self.linear_k(k).reshape(b,s_k,-1,self.d_k).transpose(1, 2)\n",
    "        values = self.linear_v(v).reshape(b,s_k,-1,self.d_k).transpose(1, 2)\n",
    "        attention = self._masked_attention(queries, keys, values, mask)\n",
    "        attention = self.linear_o(attention.transpose(1, 2).reshape(b,s_q,-1))\n",
    "        return attention  #-->batch_size,query_seq_length,d_model"
   ],
   "id": "4f1e8bd01f095082",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 残差链接以及归一化",
   "id": "ea52f3457abf047c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T06:22:57.274750Z",
     "start_time": "2025-04-17T06:22:57.254563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AddAndNorm(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    def forward(self, input,output):\n",
    "        return self.layernorm(self.dropout(output) + input)"
   ],
   "id": "a850384fb3d02b71",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 用到的特殊MLP",
   "id": "c4f56b638eeba1c1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T06:22:57.306208Z",
     "start_time": "2025-04-17T06:22:57.294750Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "4d1191b63f764d11",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T06:22:57.352053Z",
     "start_time": "2025-04-17T06:22:57.332848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, list_dims, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i in range(len(list_dims)-1):\n",
    "            self.layers.append(torch.nn.Linear(list_dims[i], list_dims[i+1]))\n",
    "            self.layers.append(torch.nn.ReLU())\n",
    "            self.layers.append(torch.nn.Dropout(p=dropout))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x\n",
    "        for layer in self.layers:\n",
    "            output = layer(output)\n",
    "        return output"
   ],
   "id": "d97ee46571ad7b2a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 位置编码",
   "id": "baf1452cb9025b2b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T06:22:57.425515Z",
     "start_time": "2025-04-17T06:22:57.393803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEncoding(nn.Module):  #-->这里要注意，dmodel一定要是偶数，不然我也不知道会发生什么\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.pe = torch.zeros(1,max_len, d_model)\n",
    "        self.position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) #-->max_len,1\n",
    "        self.sin_div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        self.cos_div_term = torch.exp(torch.arange(1, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        self.pe[:,:,0::2] = torch.sin(self.position * self.sin_div_term)\n",
    "        self.pe[:,:,1::2] = torch.cos(self.position * self.cos_div_term)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #处理一下设备的问题\n",
    "        self.pe = self.pe.to(x.device)\n",
    "        return x + self.pe[:,:x.size(1),:]"
   ],
   "id": "976a15f4c7244012",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 编码层",
   "id": "ad88c4bca69dfc02"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T06:22:57.522636Z",
     "start_time": "2025-04-17T06:22:57.505638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads,d_ff,max_nb_vars = 7,dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.mlp = MLP([d_model*max_nb_vars,d_model,d_model], dropout)\n",
    "        self.attention = MultiHeadAttention(n_heads, d_model)\n",
    "        self.ffn = FFN(d_model, d_ff, d_model)\n",
    "        self.add_norm1 = AddAndNorm(d_model)\n",
    "        self.add_norm2 = AddAndNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        #x的维度应该是（batch_size,nb_samples,max_nb_vars,d_model）  已经用cell-mlp处理了\n",
    "        x_flat = torch.flatten(x, start_dim=2)\n",
    "\n",
    "        mlp_output = self.mlp(x_flat)\n",
    "        #上面的部分时模型专属，处理出来的x(batch_size,nb_samples,d_model)\n",
    "\n",
    "        attention_output = self.attention(mlp_output,mlp_output,mlp_output, mask).unsqueeze(2)\n",
    "\n",
    "        x1 = self.add_norm1(x, attention_output)\n",
    "        ffn_output = self.ffn(x1)\n",
    "        encoder_output = self.add_norm2(x1, ffn_output)\n",
    "        return encoder_output"
   ],
   "id": "e59bd41de0313d92",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T06:22:57.599065Z",
     "start_time": "2025-04-17T06:22:57.576941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads,d_ff):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(n_heads, d_model)\n",
    "        self.cross_attention = MultiHeadAttention(n_heads, d_model)\n",
    "\n",
    "        self.add_norm1 = AddAndNorm(d_model)\n",
    "        self.add_norm2 = AddAndNorm(d_model)\n",
    "        self.add_norm3 = AddAndNorm(d_model)\n",
    "\n",
    "        self.ffn = FFN(d_model, d_ff, d_model)\n",
    "\n",
    "    def forward(self, memory, tgt ,src_mask=None,tgt_mask=None):\n",
    "        m = memory\n",
    "        self_attention_output = self.self_attention(tgt,tgt,tgt, tgt_mask)\n",
    "        y1 = self.add_norm1(tgt, self_attention_output)\n",
    "        cross_attention_output = self.cross_attention(tgt,m,m,src_mask)\n",
    "        y2 = self.add_norm2(y1,cross_attention_output)\n",
    "        ffn_output = self.ffn(y2)\n",
    "        decoder_output = self.add_norm3(y2,ffn_output)\n",
    "        return decoder_output"
   ],
   "id": "90bbb56f657631d4",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T06:22:57.629790Z",
     "start_time": "2025-04-17T06:22:57.615479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads,d_ff,N_encoder_layers,dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.ModuleList()\n",
    "        for i in range(N_encoder_layers):\n",
    "            self.encoder.append(EncoderLayer(d_model, n_heads, d_ff))\n",
    "\n",
    "        self.last_mlp = MLP([d_model, d_model], dropout)\n",
    "        self.cell_mlp = MLP([1, d_model,d_model], dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        #原始的x形状  batch_size,nb_samples,max_nb_vars\n",
    "        x = self.cell_mlp(x.unsqueeze(-1))\n",
    "        #现在的x形状  batch_size,nb_samples,max_nb_vars,d_model\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        x = self.last_mlp(x)\n",
    "        #现在的x形状不改变\n",
    "        x = torch.max(x,dim=1)[0]\n",
    "        #这是我们失去了第一维度，于是batch_size,max_nb_vars,d_model\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads,d_ff,N_decoder_layers):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.ModuleList()\n",
    "        for i in range(N_decoder_layers):\n",
    "            self.decoder.append(DecoderLayer(d_model, n_heads, d_ff))\n",
    "\n",
    "    def forward(self, enc_out ,tgt , src_mask=None,tgt_mask=None):\n",
    "        for layer in self.decoder:\n",
    "            tgt = layer(enc_out,tgt ,src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "\n",
    "        return tgt\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads,d_ff,vocab_size,N_encoder_layers,N_decoder_layers,dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(d_model, n_heads,d_ff,N_encoder_layers,dropout)\n",
    "        self.decoder = Decoder(d_model, n_heads,d_ff,N_decoder_layers)\n",
    "        self.output_layer = nn.Sequential(nn.Linear(d_model, vocab_size),\n",
    "                                          )\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model, padding_idx=22)\n",
    "\n",
    "    def forward(self,src,tgt,src_mask=None,tgt_mask=None):\n",
    "        tgt = self.embedding(tgt)\n",
    "        memory = self.encoder(src,src_mask)\n",
    "        decoder_output = self.decoder(memory,tgt,src_mask = src_mask,tgt_mask=tgt_mask)\n",
    "        output = self.output_layer(decoder_output)\n",
    "        return output  #-->batch_size,query_seq_length,vocab_size,按照我们的模型来讲就是b,32,21"
   ],
   "id": "6bee8216ef434c6b",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T06:22:58.355056Z",
     "start_time": "2025-04-17T06:22:57.663584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "decoder_vocab = np.array([\n",
    "    ['add', 4, 2],  # 二元操作符：加法\n",
    "    ['mul', 6, 2],  # 二元操作符：乘法\n",
    "    ['sub', 3, 2],\n",
    "    ['sin', 1, 1],  # 一元操作符：正弦函数\n",
    "    ['cos', 1, 1],  # 一元操作符：余弦函数\n",
    "    ['log', 2, 1],  # 一元操作符：对数\n",
    "    ['exp', 2, 1],  # 一元操作符：指数\n",
    "    ['neg', 0, 1],  # 一元操作符：取负（权重为0表示此处不做采样，可根据需要调整）\n",
    "    ['inv', 3, 1],  # 一元操作符：求倒数\n",
    "    ['sq', 2, 1],   # 一元操作符：平方\n",
    "    ['cb', 0, 1],   # 一元操作符：立方（权重为0暂不采样）\n",
    "    ['sqrt', 2, 1], # 一元操作符：平方根\n",
    "    ['cbrt', 0, 1], # 一元操作符：立方根（权重为0暂不采样）\n",
    "    ['C', 8, 0],    # 叶子节点：常数\n",
    "    ['x1', 8, 0],   # 叶子节点：变量1\n",
    "    ['x2', 8, 0],   # 叶子节点：变量2\n",
    "    ['x3', 4, 0],   # 叶子节点：变量3\n",
    "    ['x4', 4, 0],   # 叶子节点：变量4\n",
    "    ['x5', 2, 0],   # 叶子节点：变量5\n",
    "    ['x6', 2, 0],   # 叶子节点：变量6\n",
    "    ['<SOS>',0,0],\n",
    "    ['<EOS>',0,0],\n",
    "    ['<PAD>',0,0],\n",
    "])\n",
    "\n",
    "\n",
    "from _utils import *\n",
    "decoder_vocab = create_id_vocab(decoder_vocab)\n",
    "decoder_vocab['<PAD>']\n"
   ],
   "id": "683ed4d221d176e6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T06:22:58.385687Z",
     "start_time": "2025-04-17T06:22:58.367208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader , Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,):\n",
    "        self.datasets = datasets_generator(N_orig=1000,repeat_sampling=10)\n",
    "        self.datas = [item[0] for item in self.datasets]\n",
    "        self.labels = [item[1] for item in self.datasets]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = torch.tensor(self.datas[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return data, label\n",
    "\n",
    "\n",
    "def my_collate_fn(batch, pad_idx=22, max_length=32):\n",
    "    # batch 是一个 list，每个元素是 (data, label)\n",
    "    datas, labels = zip(*batch)  # 解包\n",
    "    datas = torch.stack([torch.tensor(d, dtype=torch.float32) for d in datas])\n",
    "\n",
    "\n",
    "\n",
    "    # padding 到 max_length\n",
    "    padded_labels = []\n",
    "    for seq in labels:\n",
    "        seq = torch.tensor(seq, dtype=torch.long)\n",
    "        if len(seq) >= max_length:\n",
    "            padded_seq = seq[:max_length]\n",
    "        else:\n",
    "            padding = [pad_idx] * (max_length - len(seq))\n",
    "            padded_seq = torch.cat([seq, torch.tensor(padding, dtype=torch.long)])\n",
    "        padded_labels.append(padded_seq)\n",
    "\n",
    "    padded_labels = torch.stack(padded_labels)\n",
    "\n",
    "    return datas, padded_labels\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "ce6e9115b1bb1810",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T06:23:05.678173Z",
     "start_time": "2025-04-17T06:22:58.403686Z"
    }
   },
   "cell_type": "code",
   "source": "loader =  DataLoader(MyDataset(),batch_size=batch_size,shuffle=True,collate_fn=my_collate_fn)",
   "id": "3d233dd071057f74",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始生成表达式数： 1000\n",
      "过滤后表达式数： 101\n",
      "去重后唯一表达式数： 97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<lambdifygenerated-31>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return x4*exp(C**2*x1*(x6**2 + 1)/x6)/(-log(x1) - 1/2*log(x2))\n",
      "<lambdifygenerated-32>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return x4*exp(C**2*x1*(x6**2 + 1)/x6)/(-log(x1) - 1/2*log(x2))\n",
      "<lambdifygenerated-33>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return x4*exp(C**2*x1*(x6**2 + 1)/x6)/(-log(x1) - 1/2*log(x2))\n",
      "<lambdifygenerated-34>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return x4*exp(C**2*x1*(x6**2 + 1)/x6)/(-log(x1) - 1/2*log(x2))\n",
      "<lambdifygenerated-35>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return x4*exp(C**2*x1*(x6**2 + 1)/x6)/(-log(x1) - 1/2*log(x2))\n",
      "<lambdifygenerated-36>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return x4*exp(C**2*x1*(x6**2 + 1)/x6)/(-log(x1) - 1/2*log(x2))\n",
      "<lambdifygenerated-37>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return x4*exp(C**2*x1*(x6**2 + 1)/x6)/(-log(x1) - 1/2*log(x2))\n",
      "<lambdifygenerated-38>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return x4*exp(C**2*x1*(x6**2 + 1)/x6)/(-log(x1) - 1/2*log(x2))\n",
      "<lambdifygenerated-39>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return x4*exp(C**2*x1*(x6**2 + 1)/x6)/(-log(x1) - 1/2*log(x2))\n",
      "<lambdifygenerated-40>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return x4*exp(C**2*x1*(x6**2 + 1)/x6)/(-log(x1) - 1/2*log(x2))\n",
      "<lambdifygenerated-71>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return (C*x2*(sqrt(x2) + x2) + x1)*exp(x1**2*x3**2)\n",
      "<lambdifygenerated-72>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return (C*x2*(sqrt(x2) + x2) + x1)*exp(x1**2*x3**2)\n",
      "<lambdifygenerated-73>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return (C*x2*(sqrt(x2) + x2) + x1)*exp(x1**2*x3**2)\n",
      "<lambdifygenerated-76>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return (C*x2*(sqrt(x2) + x2) + x1)*exp(x1**2*x3**2)\n",
      "<lambdifygenerated-77>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return (C*x2*(sqrt(x2) + x2) + x1)*exp(x1**2*x3**2)\n",
      "<lambdifygenerated-78>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return (C*x2*(sqrt(x2) + x2) + x1)*exp(x1**2*x3**2)\n",
      "<lambdifygenerated-79>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return (C*x2*(sqrt(x2) + x2) + x1)*exp(x1**2*x3**2)\n",
      "<lambdifygenerated-80>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return (C*x2*(sqrt(x2) + x2) + x1)*exp(x1**2*x3**2)\n",
      "<lambdifygenerated-112>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return sqrt(x3 + log(C))\n",
      "<lambdifygenerated-114>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return sqrt(x3 + log(C))\n",
      "<lambdifygenerated-115>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return sqrt(x3 + log(C))\n",
      "<lambdifygenerated-117>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return sqrt(x3 + log(C))\n",
      "<lambdifygenerated-119>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return sqrt(x3 + log(C))\n",
      "<lambdifygenerated-120>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return sqrt(x3 + log(C))\n",
      "<lambdifygenerated-141>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return C**2*sqrt(log(x4))\n",
      "<lambdifygenerated-142>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return C**2*sqrt(log(x4))\n",
      "<lambdifygenerated-143>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return C**2*sqrt(log(x4))\n",
      "<lambdifygenerated-144>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return C**2*sqrt(log(x4))\n",
      "<lambdifygenerated-145>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return C**2*sqrt(log(x4))\n",
      "<lambdifygenerated-146>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return C**2*sqrt(log(x4))\n",
      "<lambdifygenerated-147>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return C**2*sqrt(log(x4))\n",
      "<lambdifygenerated-148>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return C**2*sqrt(log(x4))\n",
      "<lambdifygenerated-149>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return C**2*sqrt(log(x4))\n",
      "<lambdifygenerated-150>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return C**2*sqrt(log(x4))\n",
      "<lambdifygenerated-192>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return log(C*x2)\n",
      "<lambdifygenerated-193>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return log(C*x2)\n",
      "<lambdifygenerated-196>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return log(C*x2)\n",
      "<lambdifygenerated-197>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return log(C*x2)\n",
      "<lambdifygenerated-231>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return log(C)*log(x4)\n",
      "<lambdifygenerated-232>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return log(C)*log(x4)\n",
      "<lambdifygenerated-234>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return log(C)*log(x4)\n",
      "<lambdifygenerated-235>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return log(C)*log(x4)\n",
      "<lambdifygenerated-236>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return log(C)*log(x4)\n",
      "<lambdifygenerated-240>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return log(C)*log(x4)\n",
      "<lambdifygenerated-242>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return sqrt(C + x1*x5)\n",
      "<lambdifygenerated-248>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return sqrt(C + x1*x5)\n",
      "<lambdifygenerated-249>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return sqrt(C + x1*x5)\n",
      "<lambdifygenerated-250>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return sqrt(C + x1*x5)\n",
      "<lambdifygenerated-271>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return x1*(x1 + exp(C**2*x5**2))\n",
      "<lambdifygenerated-272>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return x1*(x1 + exp(C**2*x5**2))\n",
      "<lambdifygenerated-273>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return x1*(x1 + exp(C**2*x5**2))\n",
      "<lambdifygenerated-274>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return x1*(x1 + exp(C**2*x5**2))\n",
      "<lambdifygenerated-275>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return x1*(x1 + exp(C**2*x5**2))\n",
      "<lambdifygenerated-276>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return x1*(x1 + exp(C**2*x5**2))\n",
      "<lambdifygenerated-277>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return x1*(x1 + exp(C**2*x5**2))\n",
      "<lambdifygenerated-278>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return x1*(x1 + exp(C**2*x5**2))\n",
      "<lambdifygenerated-279>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return x1*(x1 + exp(C**2*x5**2))\n",
      "<lambdifygenerated-280>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return x1*(x1 + exp(C**2*x5**2))\n",
      "<lambdifygenerated-311>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return sqrt(C)*sqrt(C*cos(x1) + 1) + C*x3 + x2\n",
      "<lambdifygenerated-312>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return sqrt(C)*sqrt(C*cos(x1) + 1) + C*x3 + x2\n",
      "<lambdifygenerated-313>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return sqrt(C)*sqrt(C*cos(x1) + 1) + C*x3 + x2\n",
      "<lambdifygenerated-314>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return sqrt(C)*sqrt(C*cos(x1) + 1) + C*x3 + x2\n",
      "<lambdifygenerated-315>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return sqrt(C)*sqrt(C*cos(x1) + 1) + C*x3 + x2\n",
      "<lambdifygenerated-316>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return sqrt(C)*sqrt(C*cos(x1) + 1) + C*x3 + x2\n",
      "<lambdifygenerated-317>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return sqrt(C)*sqrt(C*cos(x1) + 1) + C*x3 + x2\n",
      "<lambdifygenerated-318>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return sqrt(C)*sqrt(C*cos(x1) + 1) + C*x3 + x2\n",
      "<lambdifygenerated-319>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return sqrt(C)*sqrt(C*cos(x1) + 1) + C*x3 + x2\n",
      "<lambdifygenerated-320>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return sqrt(C)*sqrt(C*cos(x1) + 1) + C*x3 + x2\n",
      "<lambdifygenerated-321>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return log(C*x3)\n",
      "<lambdifygenerated-322>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return log(C*x3)\n",
      "<lambdifygenerated-329>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return log(C*x3)\n",
      "<lambdifygenerated-331>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return log(C**2*(C + x1 + x6))**(-1.0)\n",
      "<lambdifygenerated-332>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return log(C**2*(C + x1 + x6))**(-1.0)\n",
      "<lambdifygenerated-336>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return log(C**2*(C + x1 + x6))**(-1.0)\n",
      "<lambdifygenerated-338>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return log(C**2*(C + x1 + x6))**(-1.0)\n",
      "<lambdifygenerated-361>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return x6 + 2*log(C)\n",
      "<lambdifygenerated-362>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return x6 + 2*log(C)\n",
      "<lambdifygenerated-363>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return x6 + 2*log(C)\n",
      "<lambdifygenerated-370>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return x6 + 2*log(C)\n",
      "<lambdifygenerated-371>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return x1 + x2*exp(-C*x2) + x3\n",
      "<lambdifygenerated-451>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return x2*(x2*x4**2 + log(x4*(x2 + x3 + sin(C)) + cos(x1)))*exp(x3)\n",
      "<lambdifygenerated-453>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return x2*(x2*x4**2 + log(x4*(x2 + x3 + sin(C)) + cos(x1)))*exp(x3)\n",
      "<lambdifygenerated-454>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return x2*(x2*x4**2 + log(x4*(x2 + x3 + sin(C)) + cos(x1)))*exp(x3)\n",
      "<lambdifygenerated-455>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return x2*(x2*x4**2 + log(x4*(x2 + x3 + sin(C)) + cos(x1)))*exp(x3)\n",
      "<lambdifygenerated-456>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return x2*(x2*x4**2 + log(x4*(x2 + x3 + sin(C)) + cos(x1)))*exp(x3)\n",
      "<lambdifygenerated-457>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return x2*(x2*x4**2 + log(x4*(x2 + x3 + sin(C)) + cos(x1)))*exp(x3)\n",
      "<lambdifygenerated-458>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return x2*(x2*x4**2 + log(x4*(x2 + x3 + sin(C)) + cos(x1)))*exp(x3)\n",
      "<lambdifygenerated-459>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return x2*(x2*x4**2 + log(x4*(x2 + x3 + sin(C)) + cos(x1)))*exp(x3)\n",
      "<lambdifygenerated-460>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return x2*(x2*x4**2 + log(x4*(x2 + x3 + sin(C)) + cos(x1)))*exp(x3)\n",
      "<lambdifygenerated-471>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return sqrt(sqrt(C) + 4*x2**2)\n",
      "<lambdifygenerated-472>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return sqrt(sqrt(C) + 4*x2**2)\n",
      "<lambdifygenerated-473>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return sqrt(sqrt(C) + 4*x2**2)\n",
      "<lambdifygenerated-475>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return sqrt(sqrt(C) + 4*x2**2)\n",
      "<lambdifygenerated-477>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return sqrt(sqrt(C) + 4*x2**2)\n",
      "<lambdifygenerated-491>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return -x4*log(C + x4)\n",
      "<lambdifygenerated-493>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return -x4*log(C + x4)\n",
      "<lambdifygenerated-496>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return -x4*log(C + x4)\n",
      "<lambdifygenerated-497>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return -x4*log(C + x4)\n",
      "<lambdifygenerated-498>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return -x4*log(C + x4)\n",
      "<lambdifygenerated-511>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return exp(C*x1**2)\n",
      "<lambdifygenerated-513>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return exp(C*x1**2)\n",
      "<lambdifygenerated-515>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return exp(C*x1**2)\n",
      "<lambdifygenerated-516>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return exp(C*x1**2)\n",
      "<lambdifygenerated-581>:2: RuntimeWarning: overflow encountered in power\n",
      "  return log((x5*x6)**(C*x3*x4*sin(x2)))\n",
      "<lambdifygenerated-581>:2: RuntimeWarning: divide by zero encountered in log\n",
      "  return log((x5*x6)**(C*x3*x4*sin(x2)))\n",
      "<lambdifygenerated-582>:2: RuntimeWarning: overflow encountered in power\n",
      "  return log((x5*x6)**(C*x3*x4*sin(x2)))\n",
      "<lambdifygenerated-582>:2: RuntimeWarning: divide by zero encountered in log\n",
      "  return log((x5*x6)**(C*x3*x4*sin(x2)))\n",
      "<lambdifygenerated-583>:2: RuntimeWarning: overflow encountered in power\n",
      "  return log((x5*x6)**(C*x3*x4*sin(x2)))\n",
      "<lambdifygenerated-583>:2: RuntimeWarning: divide by zero encountered in log\n",
      "  return log((x5*x6)**(C*x3*x4*sin(x2)))\n",
      "<lambdifygenerated-584>:2: RuntimeWarning: overflow encountered in power\n",
      "  return log((x5*x6)**(C*x3*x4*sin(x2)))\n",
      "<lambdifygenerated-584>:2: RuntimeWarning: divide by zero encountered in log\n",
      "  return log((x5*x6)**(C*x3*x4*sin(x2)))\n",
      "<lambdifygenerated-585>:2: RuntimeWarning: overflow encountered in power\n",
      "  return log((x5*x6)**(C*x3*x4*sin(x2)))\n",
      "<lambdifygenerated-585>:2: RuntimeWarning: divide by zero encountered in log\n",
      "  return log((x5*x6)**(C*x3*x4*sin(x2)))\n",
      "<lambdifygenerated-586>:2: RuntimeWarning: overflow encountered in power\n",
      "  return log((x5*x6)**(C*x3*x4*sin(x2)))\n",
      "<lambdifygenerated-587>:2: RuntimeWarning: overflow encountered in power\n",
      "  return log((x5*x6)**(C*x3*x4*sin(x2)))\n",
      "<lambdifygenerated-587>:2: RuntimeWarning: divide by zero encountered in log\n",
      "  return log((x5*x6)**(C*x3*x4*sin(x2)))\n",
      "<lambdifygenerated-588>:2: RuntimeWarning: divide by zero encountered in log\n",
      "  return log((x5*x6)**(C*x3*x4*sin(x2)))\n",
      "<lambdifygenerated-589>:2: RuntimeWarning: divide by zero encountered in log\n",
      "  return log((x5*x6)**(C*x3*x4*sin(x2)))\n",
      "<lambdifygenerated-590>:2: RuntimeWarning: overflow encountered in power\n",
      "  return log((x5*x6)**(C*x3*x4*sin(x2)))\n",
      "<lambdifygenerated-590>:2: RuntimeWarning: divide by zero encountered in log\n",
      "  return log((x5*x6)**(C*x3*x4*sin(x2)))\n",
      "<lambdifygenerated-821>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return x3*exp(x3*x4*(C + x2)*(C + x1*sqrt(cos(x3)) + x1) + cos(x1 + x2**(-1.0)))\n",
      "<lambdifygenerated-821>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return x3*exp(x3*x4*(C + x2)*(C + x1*sqrt(cos(x3)) + x1) + cos(x1 + x2**(-1.0)))\n",
      "<lambdifygenerated-822>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return x3*exp(x3*x4*(C + x2)*(C + x1*sqrt(cos(x3)) + x1) + cos(x1 + x2**(-1.0)))\n",
      "<lambdifygenerated-822>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return x3*exp(x3*x4*(C + x2)*(C + x1*sqrt(cos(x3)) + x1) + cos(x1 + x2**(-1.0)))\n",
      "<lambdifygenerated-823>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return x3*exp(x3*x4*(C + x2)*(C + x1*sqrt(cos(x3)) + x1) + cos(x1 + x2**(-1.0)))\n",
      "<lambdifygenerated-823>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return x3*exp(x3*x4*(C + x2)*(C + x1*sqrt(cos(x3)) + x1) + cos(x1 + x2**(-1.0)))\n",
      "<lambdifygenerated-824>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return x3*exp(x3*x4*(C + x2)*(C + x1*sqrt(cos(x3)) + x1) + cos(x1 + x2**(-1.0)))\n",
      "<lambdifygenerated-824>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return x3*exp(x3*x4*(C + x2)*(C + x1*sqrt(cos(x3)) + x1) + cos(x1 + x2**(-1.0)))\n",
      "<lambdifygenerated-825>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return x3*exp(x3*x4*(C + x2)*(C + x1*sqrt(cos(x3)) + x1) + cos(x1 + x2**(-1.0)))\n",
      "<lambdifygenerated-825>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return x3*exp(x3*x4*(C + x2)*(C + x1*sqrt(cos(x3)) + x1) + cos(x1 + x2**(-1.0)))\n",
      "<lambdifygenerated-826>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return x3*exp(x3*x4*(C + x2)*(C + x1*sqrt(cos(x3)) + x1) + cos(x1 + x2**(-1.0)))\n",
      "<lambdifygenerated-826>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return x3*exp(x3*x4*(C + x2)*(C + x1*sqrt(cos(x3)) + x1) + cos(x1 + x2**(-1.0)))\n",
      "<lambdifygenerated-827>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return x3*exp(x3*x4*(C + x2)*(C + x1*sqrt(cos(x3)) + x1) + cos(x1 + x2**(-1.0)))\n",
      "<lambdifygenerated-827>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return x3*exp(x3*x4*(C + x2)*(C + x1*sqrt(cos(x3)) + x1) + cos(x1 + x2**(-1.0)))\n",
      "<lambdifygenerated-828>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return x3*exp(x3*x4*(C + x2)*(C + x1*sqrt(cos(x3)) + x1) + cos(x1 + x2**(-1.0)))\n",
      "<lambdifygenerated-828>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return x3*exp(x3*x4*(C + x2)*(C + x1*sqrt(cos(x3)) + x1) + cos(x1 + x2**(-1.0)))\n",
      "<lambdifygenerated-829>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return x3*exp(x3*x4*(C + x2)*(C + x1*sqrt(cos(x3)) + x1) + cos(x1 + x2**(-1.0)))\n",
      "<lambdifygenerated-829>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return x3*exp(x3*x4*(C + x2)*(C + x1*sqrt(cos(x3)) + x1) + cos(x1 + x2**(-1.0)))\n",
      "<lambdifygenerated-830>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return x3*exp(x3*x4*(C + x2)*(C + x1*sqrt(cos(x3)) + x1) + cos(x1 + x2**(-1.0)))\n",
      "<lambdifygenerated-830>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return x3*exp(x3*x4*(C + x2)*(C + x1*sqrt(cos(x3)) + x1) + cos(x1 + x2**(-1.0)))\n",
      "<lambdifygenerated-831>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return x1**2*exp(sqrt(C + x6**2))\n",
      "<lambdifygenerated-834>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return x1**2*exp(sqrt(C + x6**2))\n",
      "<lambdifygenerated-835>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return x1**2*exp(sqrt(C + x6**2))\n",
      "<lambdifygenerated-838>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return x1**2*exp(sqrt(C + x6**2))\n",
      "<lambdifygenerated-840>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return x1**2*exp(sqrt(C + x6**2))\n",
      "<lambdifygenerated-852>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return x5*log(C)\n",
      "<lambdifygenerated-853>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return x5*log(C)\n",
      "<lambdifygenerated-854>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return x5*log(C)\n",
      "<lambdifygenerated-891>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return log(C*x5)\n",
      "<lambdifygenerated-896>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return log(C*x5)\n",
      "<lambdifygenerated-898>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return log(C*x5)\n",
      "<lambdifygenerated-899>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return log(C*x5)\n",
      "<lambdifygenerated-900>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return log(C*x5)\n",
      "<lambdifygenerated-911>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return sin(sqrt(x2)*sqrt(x6**2 + 1)*sqrt(C**2*x3 + 1)*exp((1/4)*x2)/(sqrt(C)*sqrt(x3)*sqrt(x6)))\n",
      "<lambdifygenerated-912>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return sin(sqrt(x2)*sqrt(x6**2 + 1)*sqrt(C**2*x3 + 1)*exp((1/4)*x2)/(sqrt(C)*sqrt(x3)*sqrt(x6)))\n",
      "<lambdifygenerated-913>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return sin(sqrt(x2)*sqrt(x6**2 + 1)*sqrt(C**2*x3 + 1)*exp((1/4)*x2)/(sqrt(C)*sqrt(x3)*sqrt(x6)))\n",
      "<lambdifygenerated-921>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return log(C)*log(x2 + sin(C))**2*cos(C)\n",
      "<lambdifygenerated-922>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return log(C)*log(x2 + sin(C))**2*cos(C)\n",
      "<lambdifygenerated-923>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return log(C)*log(x2 + sin(C))**2*cos(C)\n",
      "<lambdifygenerated-925>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return log(C)*log(x2 + sin(C))**2*cos(C)\n",
      "<lambdifygenerated-927>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return log(C)*log(x2 + sin(C))**2*cos(C)\n",
      "<lambdifygenerated-928>:2: RuntimeWarning: invalid value encountered in log\n",
      "  return log(C)*log(x2 + sin(C))**2*cos(C)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "采样到的数据集数： 713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<lambdifygenerated-953>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return sqrt(C) + x3 + x4\n",
      "<lambdifygenerated-954>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return sqrt(C) + x3 + x4\n",
      "<lambdifygenerated-955>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return sqrt(C) + x3 + x4\n",
      "<lambdifygenerated-956>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return sqrt(C) + x3 + x4\n",
      "<lambdifygenerated-957>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return sqrt(C) + x3 + x4\n",
      "<lambdifygenerated-959>:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return sqrt(C) + x3 + x4\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T06:23:07.920188Z",
     "start_time": "2025-04-17T06:23:05.714489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = Transformer(d_model =d_model,n_heads = 4, d_ff = d_ff,N_encoder_layers=N_encoder_layers,N_decoder_layers = N_decoder_layers,vocab_size=  vocab_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=22)\n",
    "optimizer = Adam(model.parameters(),lr = 0.0001)"
   ],
   "id": "821de10cf82a6a0b",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T06:23:07.951317Z",
     "start_time": "2025-04-17T06:23:07.937334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "def train(num_epochs):\n",
    "    model.train()\n",
    "    pad_id = decoder_vocab['<PAD>']\n",
    "    eos_id = decoder_vocab['<EOS>']\n",
    "    sos_id = decoder_vocab['<SOS>']\n",
    "    model.train()\n",
    "    losses = []\n",
    "    epoches = torch.arange(1, num_epochs+1)\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for idx, (x,y) in enumerate(loader):\n",
    "            tem_batch_size = y.size(0)\n",
    "            pad_idx = torch.where(y == pad_id)[0][0]\n",
    "            eos = torch.full((tem_batch_size,1), eos_id, dtype=torch.long)\n",
    "            sos = torch.full((tem_batch_size,1), sos_id, dtype=torch.long)\n",
    "\n",
    "\n",
    "            labels = torch.cat([y[:,:pad_idx] ,eos,y[:,pad_idx+1:]],dim = 1)\n",
    "            labels = labels\n",
    "            decoder_input = torch.cat([sos,y[:,:-1]],dim = 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            x,labels,decoder_input = x.to(device),labels.to(device),decoder_input.to(device)\n",
    "            y_pred = model(x,decoder_input)\n",
    "\n",
    "\n",
    "            y_pred = y_pred.view(-1, y_pred.size(-1))  # [batch * seq_len, vocab_size]\n",
    "            labels = labels.view(-1)                  # [batch * seq_len]\n",
    "\n",
    "            loss = criterion(y_pred, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()*x.size(0)\n",
    "            if (idx+1) % 1 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}],Batches[{idx+1}/{len(loader)}], Loss: {running_loss/((idx+1)*(x.size(0))):.3f}\")\n",
    "        losses.append(running_loss/len(loader.dataset))\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(epoches, losses)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()"
   ],
   "id": "8358ab792cbedbc3",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T06:23:08.012773Z",
     "start_time": "2025-04-17T06:23:07.968646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if torch.isnan(param).any():\n",
    "        print(f\"NaN detected in parameter: {name}\")"
   ],
   "id": "90a9d33339a51291",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-17T06:23:08.031925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 如果你在 Jupyter 或脚本里运行：\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)\n",
    "\n",
    "train(num_epochs = 1)"
   ],
   "id": "f9a6cc15072018",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.encoder.0.mlp.layers.0.weight is on cuda:0\n",
      "encoder.encoder.0.mlp.layers.0.bias is on cuda:0\n",
      "encoder.encoder.0.mlp.layers.3.weight is on cuda:0\n",
      "encoder.encoder.0.mlp.layers.3.bias is on cuda:0\n",
      "encoder.encoder.0.attention.linear_q.weight is on cuda:0\n",
      "encoder.encoder.0.attention.linear_q.bias is on cuda:0\n",
      "encoder.encoder.0.attention.linear_k.weight is on cuda:0\n",
      "encoder.encoder.0.attention.linear_k.bias is on cuda:0\n",
      "encoder.encoder.0.attention.linear_v.weight is on cuda:0\n",
      "encoder.encoder.0.attention.linear_v.bias is on cuda:0\n",
      "encoder.encoder.0.attention.linear_o.weight is on cuda:0\n",
      "encoder.encoder.0.attention.linear_o.bias is on cuda:0\n",
      "encoder.encoder.0.ffn.dense1.weight is on cuda:0\n",
      "encoder.encoder.0.ffn.dense1.bias is on cuda:0\n",
      "encoder.encoder.0.ffn.dense2.weight is on cuda:0\n",
      "encoder.encoder.0.ffn.dense2.bias is on cuda:0\n",
      "encoder.encoder.0.add_norm1.layernorm.weight is on cuda:0\n",
      "encoder.encoder.0.add_norm1.layernorm.bias is on cuda:0\n",
      "encoder.encoder.0.add_norm2.layernorm.weight is on cuda:0\n",
      "encoder.encoder.0.add_norm2.layernorm.bias is on cuda:0\n",
      "encoder.encoder.1.mlp.layers.0.weight is on cuda:0\n",
      "encoder.encoder.1.mlp.layers.0.bias is on cuda:0\n",
      "encoder.encoder.1.mlp.layers.3.weight is on cuda:0\n",
      "encoder.encoder.1.mlp.layers.3.bias is on cuda:0\n",
      "encoder.encoder.1.attention.linear_q.weight is on cuda:0\n",
      "encoder.encoder.1.attention.linear_q.bias is on cuda:0\n",
      "encoder.encoder.1.attention.linear_k.weight is on cuda:0\n",
      "encoder.encoder.1.attention.linear_k.bias is on cuda:0\n",
      "encoder.encoder.1.attention.linear_v.weight is on cuda:0\n",
      "encoder.encoder.1.attention.linear_v.bias is on cuda:0\n",
      "encoder.encoder.1.attention.linear_o.weight is on cuda:0\n",
      "encoder.encoder.1.attention.linear_o.bias is on cuda:0\n",
      "encoder.encoder.1.ffn.dense1.weight is on cuda:0\n",
      "encoder.encoder.1.ffn.dense1.bias is on cuda:0\n",
      "encoder.encoder.1.ffn.dense2.weight is on cuda:0\n",
      "encoder.encoder.1.ffn.dense2.bias is on cuda:0\n",
      "encoder.encoder.1.add_norm1.layernorm.weight is on cuda:0\n",
      "encoder.encoder.1.add_norm1.layernorm.bias is on cuda:0\n",
      "encoder.encoder.1.add_norm2.layernorm.weight is on cuda:0\n",
      "encoder.encoder.1.add_norm2.layernorm.bias is on cuda:0\n",
      "encoder.encoder.2.mlp.layers.0.weight is on cuda:0\n",
      "encoder.encoder.2.mlp.layers.0.bias is on cuda:0\n",
      "encoder.encoder.2.mlp.layers.3.weight is on cuda:0\n",
      "encoder.encoder.2.mlp.layers.3.bias is on cuda:0\n",
      "encoder.encoder.2.attention.linear_q.weight is on cuda:0\n",
      "encoder.encoder.2.attention.linear_q.bias is on cuda:0\n",
      "encoder.encoder.2.attention.linear_k.weight is on cuda:0\n",
      "encoder.encoder.2.attention.linear_k.bias is on cuda:0\n",
      "encoder.encoder.2.attention.linear_v.weight is on cuda:0\n",
      "encoder.encoder.2.attention.linear_v.bias is on cuda:0\n",
      "encoder.encoder.2.attention.linear_o.weight is on cuda:0\n",
      "encoder.encoder.2.attention.linear_o.bias is on cuda:0\n",
      "encoder.encoder.2.ffn.dense1.weight is on cuda:0\n",
      "encoder.encoder.2.ffn.dense1.bias is on cuda:0\n",
      "encoder.encoder.2.ffn.dense2.weight is on cuda:0\n",
      "encoder.encoder.2.ffn.dense2.bias is on cuda:0\n",
      "encoder.encoder.2.add_norm1.layernorm.weight is on cuda:0\n",
      "encoder.encoder.2.add_norm1.layernorm.bias is on cuda:0\n",
      "encoder.encoder.2.add_norm2.layernorm.weight is on cuda:0\n",
      "encoder.encoder.2.add_norm2.layernorm.bias is on cuda:0\n",
      "encoder.encoder.3.mlp.layers.0.weight is on cuda:0\n",
      "encoder.encoder.3.mlp.layers.0.bias is on cuda:0\n",
      "encoder.encoder.3.mlp.layers.3.weight is on cuda:0\n",
      "encoder.encoder.3.mlp.layers.3.bias is on cuda:0\n",
      "encoder.encoder.3.attention.linear_q.weight is on cuda:0\n",
      "encoder.encoder.3.attention.linear_q.bias is on cuda:0\n",
      "encoder.encoder.3.attention.linear_k.weight is on cuda:0\n",
      "encoder.encoder.3.attention.linear_k.bias is on cuda:0\n",
      "encoder.encoder.3.attention.linear_v.weight is on cuda:0\n",
      "encoder.encoder.3.attention.linear_v.bias is on cuda:0\n",
      "encoder.encoder.3.attention.linear_o.weight is on cuda:0\n",
      "encoder.encoder.3.attention.linear_o.bias is on cuda:0\n",
      "encoder.encoder.3.ffn.dense1.weight is on cuda:0\n",
      "encoder.encoder.3.ffn.dense1.bias is on cuda:0\n",
      "encoder.encoder.3.ffn.dense2.weight is on cuda:0\n",
      "encoder.encoder.3.ffn.dense2.bias is on cuda:0\n",
      "encoder.encoder.3.add_norm1.layernorm.weight is on cuda:0\n",
      "encoder.encoder.3.add_norm1.layernorm.bias is on cuda:0\n",
      "encoder.encoder.3.add_norm2.layernorm.weight is on cuda:0\n",
      "encoder.encoder.3.add_norm2.layernorm.bias is on cuda:0\n",
      "encoder.last_mlp.layers.0.weight is on cuda:0\n",
      "encoder.last_mlp.layers.0.bias is on cuda:0\n",
      "encoder.cell_mlp.layers.0.weight is on cuda:0\n",
      "encoder.cell_mlp.layers.0.bias is on cuda:0\n",
      "encoder.cell_mlp.layers.3.weight is on cuda:0\n",
      "encoder.cell_mlp.layers.3.bias is on cuda:0\n",
      "decoder.decoder.0.self_attention.linear_q.weight is on cuda:0\n",
      "decoder.decoder.0.self_attention.linear_q.bias is on cuda:0\n",
      "decoder.decoder.0.self_attention.linear_k.weight is on cuda:0\n",
      "decoder.decoder.0.self_attention.linear_k.bias is on cuda:0\n",
      "decoder.decoder.0.self_attention.linear_v.weight is on cuda:0\n",
      "decoder.decoder.0.self_attention.linear_v.bias is on cuda:0\n",
      "decoder.decoder.0.self_attention.linear_o.weight is on cuda:0\n",
      "decoder.decoder.0.self_attention.linear_o.bias is on cuda:0\n",
      "decoder.decoder.0.cross_attention.linear_q.weight is on cuda:0\n",
      "decoder.decoder.0.cross_attention.linear_q.bias is on cuda:0\n",
      "decoder.decoder.0.cross_attention.linear_k.weight is on cuda:0\n",
      "decoder.decoder.0.cross_attention.linear_k.bias is on cuda:0\n",
      "decoder.decoder.0.cross_attention.linear_v.weight is on cuda:0\n",
      "decoder.decoder.0.cross_attention.linear_v.bias is on cuda:0\n",
      "decoder.decoder.0.cross_attention.linear_o.weight is on cuda:0\n",
      "decoder.decoder.0.cross_attention.linear_o.bias is on cuda:0\n",
      "decoder.decoder.0.add_norm1.layernorm.weight is on cuda:0\n",
      "decoder.decoder.0.add_norm1.layernorm.bias is on cuda:0\n",
      "decoder.decoder.0.add_norm2.layernorm.weight is on cuda:0\n",
      "decoder.decoder.0.add_norm2.layernorm.bias is on cuda:0\n",
      "decoder.decoder.0.add_norm3.layernorm.weight is on cuda:0\n",
      "decoder.decoder.0.add_norm3.layernorm.bias is on cuda:0\n",
      "decoder.decoder.0.ffn.dense1.weight is on cuda:0\n",
      "decoder.decoder.0.ffn.dense1.bias is on cuda:0\n",
      "decoder.decoder.0.ffn.dense2.weight is on cuda:0\n",
      "decoder.decoder.0.ffn.dense2.bias is on cuda:0\n",
      "decoder.decoder.1.self_attention.linear_q.weight is on cuda:0\n",
      "decoder.decoder.1.self_attention.linear_q.bias is on cuda:0\n",
      "decoder.decoder.1.self_attention.linear_k.weight is on cuda:0\n",
      "decoder.decoder.1.self_attention.linear_k.bias is on cuda:0\n",
      "decoder.decoder.1.self_attention.linear_v.weight is on cuda:0\n",
      "decoder.decoder.1.self_attention.linear_v.bias is on cuda:0\n",
      "decoder.decoder.1.self_attention.linear_o.weight is on cuda:0\n",
      "decoder.decoder.1.self_attention.linear_o.bias is on cuda:0\n",
      "decoder.decoder.1.cross_attention.linear_q.weight is on cuda:0\n",
      "decoder.decoder.1.cross_attention.linear_q.bias is on cuda:0\n",
      "decoder.decoder.1.cross_attention.linear_k.weight is on cuda:0\n",
      "decoder.decoder.1.cross_attention.linear_k.bias is on cuda:0\n",
      "decoder.decoder.1.cross_attention.linear_v.weight is on cuda:0\n",
      "decoder.decoder.1.cross_attention.linear_v.bias is on cuda:0\n",
      "decoder.decoder.1.cross_attention.linear_o.weight is on cuda:0\n",
      "decoder.decoder.1.cross_attention.linear_o.bias is on cuda:0\n",
      "decoder.decoder.1.add_norm1.layernorm.weight is on cuda:0\n",
      "decoder.decoder.1.add_norm1.layernorm.bias is on cuda:0\n",
      "decoder.decoder.1.add_norm2.layernorm.weight is on cuda:0\n",
      "decoder.decoder.1.add_norm2.layernorm.bias is on cuda:0\n",
      "decoder.decoder.1.add_norm3.layernorm.weight is on cuda:0\n",
      "decoder.decoder.1.add_norm3.layernorm.bias is on cuda:0\n",
      "decoder.decoder.1.ffn.dense1.weight is on cuda:0\n",
      "decoder.decoder.1.ffn.dense1.bias is on cuda:0\n",
      "decoder.decoder.1.ffn.dense2.weight is on cuda:0\n",
      "decoder.decoder.1.ffn.dense2.bias is on cuda:0\n",
      "decoder.decoder.2.self_attention.linear_q.weight is on cuda:0\n",
      "decoder.decoder.2.self_attention.linear_q.bias is on cuda:0\n",
      "decoder.decoder.2.self_attention.linear_k.weight is on cuda:0\n",
      "decoder.decoder.2.self_attention.linear_k.bias is on cuda:0\n",
      "decoder.decoder.2.self_attention.linear_v.weight is on cuda:0\n",
      "decoder.decoder.2.self_attention.linear_v.bias is on cuda:0\n",
      "decoder.decoder.2.self_attention.linear_o.weight is on cuda:0\n",
      "decoder.decoder.2.self_attention.linear_o.bias is on cuda:0\n",
      "decoder.decoder.2.cross_attention.linear_q.weight is on cuda:0\n",
      "decoder.decoder.2.cross_attention.linear_q.bias is on cuda:0\n",
      "decoder.decoder.2.cross_attention.linear_k.weight is on cuda:0\n",
      "decoder.decoder.2.cross_attention.linear_k.bias is on cuda:0\n",
      "decoder.decoder.2.cross_attention.linear_v.weight is on cuda:0\n",
      "decoder.decoder.2.cross_attention.linear_v.bias is on cuda:0\n",
      "decoder.decoder.2.cross_attention.linear_o.weight is on cuda:0\n",
      "decoder.decoder.2.cross_attention.linear_o.bias is on cuda:0\n",
      "decoder.decoder.2.add_norm1.layernorm.weight is on cuda:0\n",
      "decoder.decoder.2.add_norm1.layernorm.bias is on cuda:0\n",
      "decoder.decoder.2.add_norm2.layernorm.weight is on cuda:0\n",
      "decoder.decoder.2.add_norm2.layernorm.bias is on cuda:0\n",
      "decoder.decoder.2.add_norm3.layernorm.weight is on cuda:0\n",
      "decoder.decoder.2.add_norm3.layernorm.bias is on cuda:0\n",
      "decoder.decoder.2.ffn.dense1.weight is on cuda:0\n",
      "decoder.decoder.2.ffn.dense1.bias is on cuda:0\n",
      "decoder.decoder.2.ffn.dense2.weight is on cuda:0\n",
      "decoder.decoder.2.ffn.dense2.bias is on cuda:0\n",
      "decoder.decoder.3.self_attention.linear_q.weight is on cuda:0\n",
      "decoder.decoder.3.self_attention.linear_q.bias is on cuda:0\n",
      "decoder.decoder.3.self_attention.linear_k.weight is on cuda:0\n",
      "decoder.decoder.3.self_attention.linear_k.bias is on cuda:0\n",
      "decoder.decoder.3.self_attention.linear_v.weight is on cuda:0\n",
      "decoder.decoder.3.self_attention.linear_v.bias is on cuda:0\n",
      "decoder.decoder.3.self_attention.linear_o.weight is on cuda:0\n",
      "decoder.decoder.3.self_attention.linear_o.bias is on cuda:0\n",
      "decoder.decoder.3.cross_attention.linear_q.weight is on cuda:0\n",
      "decoder.decoder.3.cross_attention.linear_q.bias is on cuda:0\n",
      "decoder.decoder.3.cross_attention.linear_k.weight is on cuda:0\n",
      "decoder.decoder.3.cross_attention.linear_k.bias is on cuda:0\n",
      "decoder.decoder.3.cross_attention.linear_v.weight is on cuda:0\n",
      "decoder.decoder.3.cross_attention.linear_v.bias is on cuda:0\n",
      "decoder.decoder.3.cross_attention.linear_o.weight is on cuda:0\n",
      "decoder.decoder.3.cross_attention.linear_o.bias is on cuda:0\n",
      "decoder.decoder.3.add_norm1.layernorm.weight is on cuda:0\n",
      "decoder.decoder.3.add_norm1.layernorm.bias is on cuda:0\n",
      "decoder.decoder.3.add_norm2.layernorm.weight is on cuda:0\n",
      "decoder.decoder.3.add_norm2.layernorm.bias is on cuda:0\n",
      "decoder.decoder.3.add_norm3.layernorm.weight is on cuda:0\n",
      "decoder.decoder.3.add_norm3.layernorm.bias is on cuda:0\n",
      "decoder.decoder.3.ffn.dense1.weight is on cuda:0\n",
      "decoder.decoder.3.ffn.dense1.bias is on cuda:0\n",
      "decoder.decoder.3.ffn.dense2.weight is on cuda:0\n",
      "decoder.decoder.3.ffn.dense2.bias is on cuda:0\n",
      "decoder.decoder.4.self_attention.linear_q.weight is on cuda:0\n",
      "decoder.decoder.4.self_attention.linear_q.bias is on cuda:0\n",
      "decoder.decoder.4.self_attention.linear_k.weight is on cuda:0\n",
      "decoder.decoder.4.self_attention.linear_k.bias is on cuda:0\n",
      "decoder.decoder.4.self_attention.linear_v.weight is on cuda:0\n",
      "decoder.decoder.4.self_attention.linear_v.bias is on cuda:0\n",
      "decoder.decoder.4.self_attention.linear_o.weight is on cuda:0\n",
      "decoder.decoder.4.self_attention.linear_o.bias is on cuda:0\n",
      "decoder.decoder.4.cross_attention.linear_q.weight is on cuda:0\n",
      "decoder.decoder.4.cross_attention.linear_q.bias is on cuda:0\n",
      "decoder.decoder.4.cross_attention.linear_k.weight is on cuda:0\n",
      "decoder.decoder.4.cross_attention.linear_k.bias is on cuda:0\n",
      "decoder.decoder.4.cross_attention.linear_v.weight is on cuda:0\n",
      "decoder.decoder.4.cross_attention.linear_v.bias is on cuda:0\n",
      "decoder.decoder.4.cross_attention.linear_o.weight is on cuda:0\n",
      "decoder.decoder.4.cross_attention.linear_o.bias is on cuda:0\n",
      "decoder.decoder.4.add_norm1.layernorm.weight is on cuda:0\n",
      "decoder.decoder.4.add_norm1.layernorm.bias is on cuda:0\n",
      "decoder.decoder.4.add_norm2.layernorm.weight is on cuda:0\n",
      "decoder.decoder.4.add_norm2.layernorm.bias is on cuda:0\n",
      "decoder.decoder.4.add_norm3.layernorm.weight is on cuda:0\n",
      "decoder.decoder.4.add_norm3.layernorm.bias is on cuda:0\n",
      "decoder.decoder.4.ffn.dense1.weight is on cuda:0\n",
      "decoder.decoder.4.ffn.dense1.bias is on cuda:0\n",
      "decoder.decoder.4.ffn.dense2.weight is on cuda:0\n",
      "decoder.decoder.4.ffn.dense2.bias is on cuda:0\n",
      "decoder.decoder.5.self_attention.linear_q.weight is on cuda:0\n",
      "decoder.decoder.5.self_attention.linear_q.bias is on cuda:0\n",
      "decoder.decoder.5.self_attention.linear_k.weight is on cuda:0\n",
      "decoder.decoder.5.self_attention.linear_k.bias is on cuda:0\n",
      "decoder.decoder.5.self_attention.linear_v.weight is on cuda:0\n",
      "decoder.decoder.5.self_attention.linear_v.bias is on cuda:0\n",
      "decoder.decoder.5.self_attention.linear_o.weight is on cuda:0\n",
      "decoder.decoder.5.self_attention.linear_o.bias is on cuda:0\n",
      "decoder.decoder.5.cross_attention.linear_q.weight is on cuda:0\n",
      "decoder.decoder.5.cross_attention.linear_q.bias is on cuda:0\n",
      "decoder.decoder.5.cross_attention.linear_k.weight is on cuda:0\n",
      "decoder.decoder.5.cross_attention.linear_k.bias is on cuda:0\n",
      "decoder.decoder.5.cross_attention.linear_v.weight is on cuda:0\n",
      "decoder.decoder.5.cross_attention.linear_v.bias is on cuda:0\n",
      "decoder.decoder.5.cross_attention.linear_o.weight is on cuda:0\n",
      "decoder.decoder.5.cross_attention.linear_o.bias is on cuda:0\n",
      "decoder.decoder.5.add_norm1.layernorm.weight is on cuda:0\n",
      "decoder.decoder.5.add_norm1.layernorm.bias is on cuda:0\n",
      "decoder.decoder.5.add_norm2.layernorm.weight is on cuda:0\n",
      "decoder.decoder.5.add_norm2.layernorm.bias is on cuda:0\n",
      "decoder.decoder.5.add_norm3.layernorm.weight is on cuda:0\n",
      "decoder.decoder.5.add_norm3.layernorm.bias is on cuda:0\n",
      "decoder.decoder.5.ffn.dense1.weight is on cuda:0\n",
      "decoder.decoder.5.ffn.dense1.bias is on cuda:0\n",
      "decoder.decoder.5.ffn.dense2.weight is on cuda:0\n",
      "decoder.decoder.5.ffn.dense2.bias is on cuda:0\n",
      "decoder.decoder.6.self_attention.linear_q.weight is on cuda:0\n",
      "decoder.decoder.6.self_attention.linear_q.bias is on cuda:0\n",
      "decoder.decoder.6.self_attention.linear_k.weight is on cuda:0\n",
      "decoder.decoder.6.self_attention.linear_k.bias is on cuda:0\n",
      "decoder.decoder.6.self_attention.linear_v.weight is on cuda:0\n",
      "decoder.decoder.6.self_attention.linear_v.bias is on cuda:0\n",
      "decoder.decoder.6.self_attention.linear_o.weight is on cuda:0\n",
      "decoder.decoder.6.self_attention.linear_o.bias is on cuda:0\n",
      "decoder.decoder.6.cross_attention.linear_q.weight is on cuda:0\n",
      "decoder.decoder.6.cross_attention.linear_q.bias is on cuda:0\n",
      "decoder.decoder.6.cross_attention.linear_k.weight is on cuda:0\n",
      "decoder.decoder.6.cross_attention.linear_k.bias is on cuda:0\n",
      "decoder.decoder.6.cross_attention.linear_v.weight is on cuda:0\n",
      "decoder.decoder.6.cross_attention.linear_v.bias is on cuda:0\n",
      "decoder.decoder.6.cross_attention.linear_o.weight is on cuda:0\n",
      "decoder.decoder.6.cross_attention.linear_o.bias is on cuda:0\n",
      "decoder.decoder.6.add_norm1.layernorm.weight is on cuda:0\n",
      "decoder.decoder.6.add_norm1.layernorm.bias is on cuda:0\n",
      "decoder.decoder.6.add_norm2.layernorm.weight is on cuda:0\n",
      "decoder.decoder.6.add_norm2.layernorm.bias is on cuda:0\n",
      "decoder.decoder.6.add_norm3.layernorm.weight is on cuda:0\n",
      "decoder.decoder.6.add_norm3.layernorm.bias is on cuda:0\n",
      "decoder.decoder.6.ffn.dense1.weight is on cuda:0\n",
      "decoder.decoder.6.ffn.dense1.bias is on cuda:0\n",
      "decoder.decoder.6.ffn.dense2.weight is on cuda:0\n",
      "decoder.decoder.6.ffn.dense2.bias is on cuda:0\n",
      "decoder.decoder.7.self_attention.linear_q.weight is on cuda:0\n",
      "decoder.decoder.7.self_attention.linear_q.bias is on cuda:0\n",
      "decoder.decoder.7.self_attention.linear_k.weight is on cuda:0\n",
      "decoder.decoder.7.self_attention.linear_k.bias is on cuda:0\n",
      "decoder.decoder.7.self_attention.linear_v.weight is on cuda:0\n",
      "decoder.decoder.7.self_attention.linear_v.bias is on cuda:0\n",
      "decoder.decoder.7.self_attention.linear_o.weight is on cuda:0\n",
      "decoder.decoder.7.self_attention.linear_o.bias is on cuda:0\n",
      "decoder.decoder.7.cross_attention.linear_q.weight is on cuda:0\n",
      "decoder.decoder.7.cross_attention.linear_q.bias is on cuda:0\n",
      "decoder.decoder.7.cross_attention.linear_k.weight is on cuda:0\n",
      "decoder.decoder.7.cross_attention.linear_k.bias is on cuda:0\n",
      "decoder.decoder.7.cross_attention.linear_v.weight is on cuda:0\n",
      "decoder.decoder.7.cross_attention.linear_v.bias is on cuda:0\n",
      "decoder.decoder.7.cross_attention.linear_o.weight is on cuda:0\n",
      "decoder.decoder.7.cross_attention.linear_o.bias is on cuda:0\n",
      "decoder.decoder.7.add_norm1.layernorm.weight is on cuda:0\n",
      "decoder.decoder.7.add_norm1.layernorm.bias is on cuda:0\n",
      "decoder.decoder.7.add_norm2.layernorm.weight is on cuda:0\n",
      "decoder.decoder.7.add_norm2.layernorm.bias is on cuda:0\n",
      "decoder.decoder.7.add_norm3.layernorm.weight is on cuda:0\n",
      "decoder.decoder.7.add_norm3.layernorm.bias is on cuda:0\n",
      "decoder.decoder.7.ffn.dense1.weight is on cuda:0\n",
      "decoder.decoder.7.ffn.dense1.bias is on cuda:0\n",
      "decoder.decoder.7.ffn.dense2.weight is on cuda:0\n",
      "decoder.decoder.7.ffn.dense2.bias is on cuda:0\n",
      "output_layer.0.weight is on cuda:0\n",
      "output_layer.0.bias is on cuda:0\n",
      "embedding.weight is on cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LiuJiTing\\AppData\\Local\\Temp\\ipykernel_20148\\1220609785.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  datas = torch.stack([torch.tensor(d, dtype=torch.float32) for d in datas])\n",
      "C:\\Users\\LiuJiTing\\AppData\\Local\\Temp\\ipykernel_20148\\1220609785.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  seq = torch.tensor(seq, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1],Batches[1/23], Loss: 3.471\n",
      "Epoch [1/1],Batches[2/23], Loss: 3.139\n",
      "Epoch [1/1],Batches[3/23], Loss: 2.951\n",
      "Epoch [1/1],Batches[4/23], Loss: 2.836\n",
      "Epoch [1/1],Batches[5/23], Loss: 2.736\n",
      "Epoch [1/1],Batches[6/23], Loss: 2.670\n",
      "Epoch [1/1],Batches[7/23], Loss: 2.629\n",
      "Epoch [1/1],Batches[8/23], Loss: 2.585\n",
      "Epoch [1/1],Batches[9/23], Loss: 2.536\n",
      "Epoch [1/1],Batches[10/23], Loss: 2.489\n",
      "Epoch [1/1],Batches[11/23], Loss: 2.443\n",
      "Epoch [1/1],Batches[12/23], Loss: 2.398\n",
      "Epoch [1/1],Batches[13/23], Loss: 2.361\n",
      "Epoch [1/1],Batches[14/23], Loss: 2.317\n",
      "Epoch [1/1],Batches[15/23], Loss: 2.286\n",
      "Epoch [1/1],Batches[16/23], Loss: 2.254\n",
      "Epoch [1/1],Batches[17/23], Loss: 2.212\n",
      "Epoch [1/1],Batches[18/23], Loss: 2.181\n",
      "Epoch [1/1],Batches[19/23], Loss: 2.156\n",
      "Epoch [1/1],Batches[20/23], Loss: 2.127\n",
      "Epoch [1/1],Batches[21/23], Loss: 2.104\n",
      "Epoch [1/1],Batches[22/23], Loss: 2.091\n",
      "Epoch [1/1],Batches[23/23], Loss: 7.188\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-17T06:28:55.448020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "x = torch.tensor([1,2,3,4])\n",
    "y = torch.tensor([2,3,4,5])\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ],
   "id": "371cec16a637cc5b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
